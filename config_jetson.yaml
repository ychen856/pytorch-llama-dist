general:
  config_type: config.yaml
  temperature: 0.6
  top_p: 0.9
  max_seq_len: 20
  max_gen_len: 64
  max_batch_size: 4
  nsamples: 128
  Hyper_m: 5
  Lamda: 0.08
  sparsity_ratio: 0.7
  use_variant: false
  splitting_point: 21

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: "silu"
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 2048
  model_type: "llama"
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pad_token_id: 0
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: null
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: "float16"
  transformers_version: "4.37.2"
  use_cache: true
  vocab_size: 32000



llama_2_7b_hf:
  #ckpt_dir_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  #tokenizer_path_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  ckpt_dir_hf: /ychen_storage/workspace/llama-2-7b-chat-hf-sep
  tokenizer_path_hf: /ychen_storage/workspace/llama-2-7b-chat-hf-sep/tokenizer.model


llama_2_7b_sep:
    ckpt_dir_hf_sep: /ychen_storage/workspace/llama-2-7b-chat-hf-sep
    tokenizer_path_hf_sep: /ychen_storage/workspace/llama-2-7b-chat-hf-sep/tokenizer.model

communication:
    server_ip: test-service.nrp-nautilus.io
    server_port: 80
    #server_ip: 104.171.203.90
    #server_port: 8000
    client_ip: 10.147.17.35
    client_port: 8000