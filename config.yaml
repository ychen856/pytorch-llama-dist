general:
  temperature: 0.6
  top_p: 0.9
  max_seq_len: 20
  max_gen_len: 64
  max_batch_size: 4
  nsamples: 128
  Hyper_m: 5
  Lamda: 0.08
  sparsity_ratio: 0.7
  use_variant: false
  splitting_point: 21

model_config:
  attention_bias: false
  attention_dropout: 0.0
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: "silu"
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 2048
  model_type: "llama"
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pad_token_id: 0
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: null
  rope_theta: 10000.0
  tie_word_embeddings: false
  torch_dtype: "float16"
  transformers_version: "4.37.2"
  use_cache: true
  vocab_size: 32000

llama_2_7b:
  ckpt_dir: /home/yichun/workspace/llama-2-7b
  tokenizer_path: /home/yichun/workspace/llama-2-7b/tokenizer.model


llama_2_7b_hf:
  #ckpt_dir_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  #tokenizer_path_hf: /home/yichun/workspace/llama-2-7b-chat-hf/
  ckpt_dir_hf: /home/ubuntu/llamaOwl/llama-2-7b-chat-hf/
  tokenizer_path_hf: /home/ubuntu/llamaOwl/llama-2-7b-chat-hf/


llama_2_7b_sep:
    ckpt_dir_sep: D:/workspace/llama-2-7b-sep
    tokenizer_path_sep: D:/workspace/llama-2-7b-sep/tokenizer.model
    #ckpt_dir_sep: /home/ubuntu/llamaOwl/llama-2-7b-sep
    #tokenizer_path_sep: /home/ubuntu/llamaOwl/llama-2-7b-sep/tokenizer.model

    ckpt_dir_sep_hf: /home/yichun/workspace/llama-2-7b-sep-hf
    tokenizer_path_sep_hf: /home/yichun/workspace/llama-2-7b-sep-hf/tokenizer.model

